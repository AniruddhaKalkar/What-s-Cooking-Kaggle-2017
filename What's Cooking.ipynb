{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a 4-Layered Neural network. Using Bag-of-Words Model. Framework: TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def open_file(file_name):\n",
    "    with open(file_name,'r') as f:\n",
    "        data=json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data=open_file(\"train.json\")\n",
    "test_data=open_file(\"test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data and Creating a bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_label_set(train_data):\n",
    "    labels = [str(i[\"cuisine\"]).lower() for i in train_data]\n",
    "    label_set = list(set(labels))\n",
    "    np.save(\"Labels.npy\",np.array(label_set))\n",
    "    return label_set\n",
    "\n",
    "def get_encoded_label(label,label_set):\n",
    "    encoded_label=np.zeros(shape=(1,len(label_set)))\n",
    "    index=label_set.index(label)\n",
    "    encoded_label[0][index]=1\n",
    "    return encoded_label.tolist()\n",
    "\n",
    "def clean_data(ingredient):\n",
    "    return re.sub(r'[\\W]', ' ', ingredient.lower())\n",
    "\n",
    "def create_bag_of_words(data,file_out=\"lexicon.npy\"):\n",
    "    bag=[]\n",
    "    for row in data:\n",
    "        ingredients=row[\"ingredients\"]\n",
    "        for ingredient in ingredients:\n",
    "            words=clean_data(ingredient).split()\n",
    "            bag+=words\n",
    "\n",
    "    bag=np.array(list(set(bag)))\n",
    "    np.save(file_out,bag)\n",
    "    return bag.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bag_of_words=create_bag_of_words(train_data,\"lexicon.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_data(in_data,bag_of_words,file_out=\"processed_train_data.npy\",flag=0):\n",
    "    data=[]\n",
    "    if flag is 0:\n",
    "        label_set = get_label_set(in_data)\n",
    "\n",
    "    for row in in_data:\n",
    "        ingredients=row[\"ingredients\"]\n",
    "        features = np.zeros(shape=(len(bag_of_words))).tolist()\n",
    "        current_words=[]\n",
    "        for ingredient in ingredients:\n",
    "            current_words += clean_data(ingredient).split()\n",
    "        for word in current_words:\n",
    "            if word in bag_of_words:\n",
    "                index_value = bag_of_words.index(word.lower())\n",
    "                features[index_value] += 1\n",
    "        if flag is 0:\n",
    "            encoded_label=get_encoded_label(str(row['cuisine']).lower(),label_set)\n",
    "            data.append([features,np.array(encoded_label)])\n",
    "        elif flag is 1:\n",
    "            data.append(features)\n",
    "    data = np.array(data)\n",
    "    shuffle(data)\n",
    "    np.save(file_out,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data(train_data,bag_of_words,\"processed_train_data.npy\",0)\n",
    "process_data(test_data,bag_of_words,\"processed_test_data.npy\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_data(train_file=\"processed_train_data.npy\",label_file=\"Labels.npy\"):\n",
    "    train_data=np.load(train_file)\n",
    "    labels=np.load(label_file)\n",
    "    return train_data,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data,labels=get_train_data()\n",
    "train_data=pd.DataFrame(train_data,columns=[\"feature_set\",\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate Training and Cross-Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=np.array(train_data[\"feature_set\"])\n",
    "Y=np.array(train_data[\"labels\"])\n",
    "X=np.array([np.array(x,dtype=np.float32) for x in X])\n",
    "Y=np.array([np.array(y,dtype=np.float32) for y in Y])\n",
    "Y=np.reshape(Y,newshape=(len(X),len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X=X[:-5000]\n",
    "train_y=Y[:-5000]\n",
    "test_X=X[-5000:]\n",
    "test_y=Y[-5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_nodes=len(X[0])\n",
    "n_nodes_1=1200\n",
    "n_nodes_2=1500\n",
    "n_nodes_3=1000\n",
    "n_classes=len(labels)\n",
    "hm_epochs=1\n",
    "batch_size=128\n",
    "n_batches=int((len(train_X))/batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=tf.placeholder('float',[None,input_nodes])\n",
    "y=tf.placeholder('float')\n",
    "hidden_layer_1={'f_num':n_nodes_1,\"weights\":tf.Variable(tf.random_normal([input_nodes,n_nodes_1])),\"bias\":tf.Variable(tf.random_normal([n_nodes_1]))}\n",
    "hidden_layer_2={'f_num':n_nodes_2,\"weights\":tf.Variable(tf.random_normal([n_nodes_1,n_nodes_2])),\"bias\":tf.Variable(tf.random_normal([n_nodes_2]))}\n",
    "hidden_layer_3={'f_num':n_nodes_3,\"weights\":tf.Variable(tf.random_normal([n_nodes_2,n_nodes_3])),\"bias\":tf.Variable(tf.random_normal([n_nodes_3]))}\n",
    "output_layer={'f_num':n_classes,\"weights\":tf.Variable(tf.random_normal([n_nodes_3,n_classes])),\"bias\":tf.Variable(tf.random_normal([n_classes]))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_network_model(data):\n",
    "    l1 = tf.add(tf.matmul(data, hidden_layer_1['weights']), hidden_layer_1['bias'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1, hidden_layer_2['weights']), hidden_layer_2['bias'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    l3 = tf.add(tf.matmul(l2, hidden_layer_3['weights']), hidden_layer_3['bias'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "\n",
    "    output = tf.matmul(l3, output_layer['weights']) + output_layer['bias']\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_neural_network(X,Y,saver):\n",
    "\n",
    "    prediction=neural_network_model(x)\n",
    "    cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss=0\n",
    "            print(str(epoch)+\"/\"+str(hm_epochs))\n",
    "            for batch in range(n_batches):\n",
    "                start=batch*batch_size\n",
    "                end=start+batch_size\n",
    "                batch_x=X[start:end]\n",
    "                batch_y=Y[start:end]\n",
    "                _,c=sess.run([optimizer,cost],feed_dict={x:batch_x,y: batch_y})\n",
    "                epoch_loss+=c\n",
    "                correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "                print(\"Batch: \"+str(batch)+\" Epoch Loss: \"+str(epoch_loss)+\" Batch Accuaracy: \"+str(accuracy.eval({x:test_X,y:test_y})))\n",
    "            correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "            print(\"Epoch: \"+str(epoch)+\" Epoch Loss: \" + str(epoch_loss) + \" Validation Accuaracy: \" + str(accuracy.eval({x:test_X,y:test_y})))\n",
    "        saver.save(sess,\"What's Cooking/model.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1\n",
      "Batch: 0 Epoch Loss: 150944.90625 Batch Accuaracy: 0.0318\n",
      "Batch: 1 Epoch Loss: 259233.015625 Batch Accuaracy: 0.0492\n",
      "Batch: 2 Epoch Loss: 364763.945313 Batch Accuaracy: 0.0756\n",
      "Batch: 3 Epoch Loss: 450674.945313 Batch Accuaracy: 0.1072\n",
      "Batch: 4 Epoch Loss: 530328.835938 Batch Accuaracy: 0.1412\n",
      "Batch: 5 Epoch Loss: 604813.40625 Batch Accuaracy: 0.165\n",
      "Batch: 6 Epoch Loss: 671933.640625 Batch Accuaracy: 0.179\n",
      "Batch: 7 Epoch Loss: 738488.546875 Batch Accuaracy: 0.1932\n",
      "Batch: 8 Epoch Loss: 807042.398438 Batch Accuaracy: 0.2062\n",
      "Batch: 9 Epoch Loss: 871345.210938 Batch Accuaracy: 0.2118\n",
      "Batch: 10 Epoch Loss: 922815.429688 Batch Accuaracy: 0.2118\n",
      "Batch: 11 Epoch Loss: 983135.234375 Batch Accuaracy: 0.2112\n",
      "Batch: 12 Epoch Loss: 1034310.30469 Batch Accuaracy: 0.2026\n",
      "Batch: 13 Epoch Loss: 1081541.03906 Batch Accuaracy: 0.2022\n",
      "Batch: 14 Epoch Loss: 1132952.52344 Batch Accuaracy: 0.2058\n",
      "Batch: 15 Epoch Loss: 1187424.63281 Batch Accuaracy: 0.2108\n",
      "Batch: 16 Epoch Loss: 1226438.55469 Batch Accuaracy: 0.2186\n",
      "Batch: 17 Epoch Loss: 1271015.5 Batch Accuaracy: 0.2268\n",
      "Batch: 18 Epoch Loss: 1313584.90625 Batch Accuaracy: 0.241\n",
      "Batch: 19 Epoch Loss: 1359479.17969 Batch Accuaracy: 0.253\n",
      "Batch: 20 Epoch Loss: 1404520.80859 Batch Accuaracy: 0.2668\n",
      "Batch: 21 Epoch Loss: 1440664.66797 Batch Accuaracy: 0.28\n",
      "Batch: 22 Epoch Loss: 1485416.16406 Batch Accuaracy: 0.289\n",
      "Batch: 23 Epoch Loss: 1529198.89063 Batch Accuaracy: 0.2974\n",
      "Batch: 24 Epoch Loss: 1568297.42578 Batch Accuaracy: 0.3052\n",
      "Batch: 25 Epoch Loss: 1617389.37891 Batch Accuaracy: 0.3096\n",
      "Batch: 26 Epoch Loss: 1653081.79297 Batch Accuaracy: 0.3162\n",
      "Batch: 27 Epoch Loss: 1691647.80859 Batch Accuaracy: 0.3224\n",
      "Batch: 28 Epoch Loss: 1726252.03906 Batch Accuaracy: 0.3282\n",
      "Batch: 29 Epoch Loss: 1766528.84375 Batch Accuaracy: 0.3308\n",
      "Batch: 30 Epoch Loss: 1805100.72656 Batch Accuaracy: 0.3344\n",
      "Batch: 31 Epoch Loss: 1844212.09766 Batch Accuaracy: 0.337\n",
      "Batch: 32 Epoch Loss: 1877936.71094 Batch Accuaracy: 0.338\n",
      "Batch: 33 Epoch Loss: 1899674.29297 Batch Accuaracy: 0.3394\n",
      "Batch: 34 Epoch Loss: 1930202.93555 Batch Accuaracy: 0.3424\n",
      "Batch: 35 Epoch Loss: 1959581.32031 Batch Accuaracy: 0.3434\n",
      "Batch: 36 Epoch Loss: 1989349.99219 Batch Accuaracy: 0.3462\n",
      "Batch: 37 Epoch Loss: 2014326.39453 Batch Accuaracy: 0.3502\n",
      "Batch: 38 Epoch Loss: 2044429.75 Batch Accuaracy: 0.3564\n",
      "Batch: 39 Epoch Loss: 2073603.84766 Batch Accuaracy: 0.3624\n",
      "Batch: 40 Epoch Loss: 2102284.79297 Batch Accuaracy: 0.3672\n",
      "Batch: 41 Epoch Loss: 2132275.37109 Batch Accuaracy: 0.37\n",
      "Batch: 42 Epoch Loss: 2163273.1875 Batch Accuaracy: 0.3716\n",
      "Batch: 43 Epoch Loss: 2187491.12305 Batch Accuaracy: 0.3774\n",
      "Batch: 44 Epoch Loss: 2214716.67773 Batch Accuaracy: 0.3806\n",
      "Batch: 45 Epoch Loss: 2240434.13086 Batch Accuaracy: 0.3832\n",
      "Batch: 46 Epoch Loss: 2266560.06055 Batch Accuaracy: 0.3874\n",
      "Batch: 47 Epoch Loss: 2293362.71289 Batch Accuaracy: 0.3874\n",
      "Batch: 48 Epoch Loss: 2321202.50195 Batch Accuaracy: 0.392\n",
      "Batch: 49 Epoch Loss: 2344893.68164 Batch Accuaracy: 0.3946\n",
      "Batch: 50 Epoch Loss: 2367741.57617 Batch Accuaracy: 0.3988\n",
      "Batch: 51 Epoch Loss: 2396704.63867 Batch Accuaracy: 0.4036\n",
      "Batch: 52 Epoch Loss: 2424221.57617 Batch Accuaracy: 0.406\n",
      "Batch: 53 Epoch Loss: 2453400.375 Batch Accuaracy: 0.4108\n",
      "Batch: 54 Epoch Loss: 2475812.74023 Batch Accuaracy: 0.4144\n",
      "Batch: 55 Epoch Loss: 2503153.82813 Batch Accuaracy: 0.4188\n",
      "Batch: 56 Epoch Loss: 2532800.19531 Batch Accuaracy: 0.4196\n",
      "Batch: 57 Epoch Loss: 2555952.08594 Batch Accuaracy: 0.4214\n",
      "Batch: 58 Epoch Loss: 2579868.40234 Batch Accuaracy: 0.4218\n",
      "Batch: 59 Epoch Loss: 2604246.30469 Batch Accuaracy: 0.4232\n",
      "Batch: 60 Epoch Loss: 2629218.74609 Batch Accuaracy: 0.4266\n",
      "Batch: 61 Epoch Loss: 2648595.38086 Batch Accuaracy: 0.4286\n",
      "Batch: 62 Epoch Loss: 2673639.78125 Batch Accuaracy: 0.4304\n",
      "Batch: 63 Epoch Loss: 2699593.03125 Batch Accuaracy: 0.434\n",
      "Batch: 64 Epoch Loss: 2721185.78516 Batch Accuaracy: 0.4358\n",
      "Batch: 65 Epoch Loss: 2743522.5625 Batch Accuaracy: 0.4368\n",
      "Batch: 66 Epoch Loss: 2762225.33203 Batch Accuaracy: 0.4382\n",
      "Batch: 67 Epoch Loss: 2788171.35547 Batch Accuaracy: 0.4382\n",
      "Batch: 68 Epoch Loss: 2809061.90625 Batch Accuaracy: 0.4366\n",
      "Batch: 69 Epoch Loss: 2832209.59961 Batch Accuaracy: 0.439\n",
      "Batch: 70 Epoch Loss: 2852348.28516 Batch Accuaracy: 0.446\n",
      "Batch: 71 Epoch Loss: 2876508.76953 Batch Accuaracy: 0.4506\n",
      "Batch: 72 Epoch Loss: 2898949.75586 Batch Accuaracy: 0.4574\n",
      "Batch: 73 Epoch Loss: 2919307.21289 Batch Accuaracy: 0.4598\n",
      "Batch: 74 Epoch Loss: 2939939.0 Batch Accuaracy: 0.464\n",
      "Batch: 75 Epoch Loss: 2966543.28125 Batch Accuaracy: 0.4676\n",
      "Batch: 76 Epoch Loss: 2986121.91797 Batch Accuaracy: 0.468\n",
      "Batch: 77 Epoch Loss: 3007501.51172 Batch Accuaracy: 0.4682\n",
      "Batch: 78 Epoch Loss: 3022841.23535 Batch Accuaracy: 0.4686\n",
      "Batch: 79 Epoch Loss: 3044712.37598 Batch Accuaracy: 0.469\n",
      "Batch: 80 Epoch Loss: 3062037.90332 Batch Accuaracy: 0.4676\n",
      "Batch: 81 Epoch Loss: 3081336.34082 Batch Accuaracy: 0.464\n",
      "Batch: 82 Epoch Loss: 3102637.41895 Batch Accuaracy: 0.4626\n",
      "Batch: 83 Epoch Loss: 3123738.85254 Batch Accuaracy: 0.463\n",
      "Batch: 84 Epoch Loss: 3142585.1377 Batch Accuaracy: 0.4634\n",
      "Batch: 85 Epoch Loss: 3161618.23535 Batch Accuaracy: 0.4652\n",
      "Batch: 86 Epoch Loss: 3180484.31348 Batch Accuaracy: 0.4668\n",
      "Batch: 87 Epoch Loss: 3199749.24707 Batch Accuaracy: 0.469\n",
      "Batch: 88 Epoch Loss: 3213269.97949 Batch Accuaracy: 0.4718\n",
      "Batch: 89 Epoch Loss: 3236318.73926 Batch Accuaracy: 0.4764\n",
      "Batch: 90 Epoch Loss: 3256359.15723 Batch Accuaracy: 0.4776\n",
      "Batch: 91 Epoch Loss: 3274073.74902 Batch Accuaracy: 0.4796\n",
      "Batch: 92 Epoch Loss: 3296039.66699 Batch Accuaracy: 0.479\n",
      "Batch: 93 Epoch Loss: 3313053.75684 Batch Accuaracy: 0.4824\n",
      "Batch: 94 Epoch Loss: 3334104.4834 Batch Accuaracy: 0.4878\n",
      "Batch: 95 Epoch Loss: 3349551.79004 Batch Accuaracy: 0.4886\n",
      "Batch: 96 Epoch Loss: 3370897.87598 Batch Accuaracy: 0.4856\n",
      "Batch: 97 Epoch Loss: 3389508.40137 Batch Accuaracy: 0.488\n",
      "Batch: 98 Epoch Loss: 3406125.16309 Batch Accuaracy: 0.4934\n",
      "Batch: 99 Epoch Loss: 3427334.23535 Batch Accuaracy: 0.4968\n",
      "Batch: 100 Epoch Loss: 3445785.2998 Batch Accuaracy: 0.4994\n",
      "Batch: 101 Epoch Loss: 3466004.78223 Batch Accuaracy: 0.499\n",
      "Batch: 102 Epoch Loss: 3487954.8584 Batch Accuaracy: 0.4992\n",
      "Batch: 103 Epoch Loss: 3507668.85449 Batch Accuaracy: 0.499\n",
      "Batch: 104 Epoch Loss: 3524657.2998 Batch Accuaracy: 0.499\n",
      "Batch: 105 Epoch Loss: 3545000.34863 Batch Accuaracy: 0.501\n",
      "Batch: 106 Epoch Loss: 3566243.19238 Batch Accuaracy: 0.5048\n",
      "Batch: 107 Epoch Loss: 3582603.28418 Batch Accuaracy: 0.5088\n",
      "Batch: 108 Epoch Loss: 3601912.32715 Batch Accuaracy: 0.515\n",
      "Batch: 109 Epoch Loss: 3618852.81348 Batch Accuaracy: 0.5152\n",
      "Batch: 110 Epoch Loss: 3636628.24121 Batch Accuaracy: 0.5154\n",
      "Batch: 111 Epoch Loss: 3648881.90723 Batch Accuaracy: 0.515\n",
      "Batch: 112 Epoch Loss: 3669356.28613 Batch Accuaracy: 0.515\n",
      "Batch: 113 Epoch Loss: 3691716.70801 Batch Accuaracy: 0.5146\n",
      "Batch: 114 Epoch Loss: 3707785.76465 Batch Accuaracy: 0.5164\n",
      "Batch: 115 Epoch Loss: 3723560.99316 Batch Accuaracy: 0.516\n",
      "Batch: 116 Epoch Loss: 3742969.62207 Batch Accuaracy: 0.5184\n",
      "Batch: 117 Epoch Loss: 3755439.28418 Batch Accuaracy: 0.5182\n",
      "Batch: 118 Epoch Loss: 3771813.05176 Batch Accuaracy: 0.5182\n",
      "Batch: 119 Epoch Loss: 3785156.47266 Batch Accuaracy: 0.5146\n",
      "Batch: 120 Epoch Loss: 3801864.28125 Batch Accuaracy: 0.5124\n",
      "Batch: 121 Epoch Loss: 3813425.45801 Batch Accuaracy: 0.5076\n",
      "Batch: 122 Epoch Loss: 3828855.25879 Batch Accuaracy: 0.5076\n",
      "Batch: 123 Epoch Loss: 3845560.96777 Batch Accuaracy: 0.5096\n",
      "Batch: 124 Epoch Loss: 3861790.33105 Batch Accuaracy: 0.5122\n",
      "Batch: 125 Epoch Loss: 3875637.54199 Batch Accuaracy: 0.5158\n",
      "Batch: 126 Epoch Loss: 3888051.67676 Batch Accuaracy: 0.5198\n",
      "Batch: 127 Epoch Loss: 3906148.22363 Batch Accuaracy: 0.526\n",
      "Batch: 128 Epoch Loss: 3922550.21777 Batch Accuaracy: 0.5312\n",
      "Batch: 129 Epoch Loss: 3941744.18262 Batch Accuaracy: 0.5322\n",
      "Batch: 130 Epoch Loss: 3954460.83008 Batch Accuaracy: 0.5322\n",
      "Batch: 131 Epoch Loss: 3972557.76758 Batch Accuaracy: 0.5324\n",
      "Batch: 132 Epoch Loss: 3989629.29492 Batch Accuaracy: 0.5326\n",
      "Batch: 133 Epoch Loss: 4001837.44629 Batch Accuaracy: 0.5342\n",
      "Batch: 134 Epoch Loss: 4019044.0791 Batch Accuaracy: 0.5338\n",
      "Batch: 135 Epoch Loss: 4034208.60059 Batch Accuaracy: 0.534\n",
      "Batch: 136 Epoch Loss: 4049499.52441 Batch Accuaracy: 0.5298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 137 Epoch Loss: 4064508.48926 Batch Accuaracy: 0.5314\n",
      "Batch: 138 Epoch Loss: 4080937.00293 Batch Accuaracy: 0.5298\n",
      "Batch: 139 Epoch Loss: 4094773.12793 Batch Accuaracy: 0.528\n",
      "Batch: 140 Epoch Loss: 4113931.89746 Batch Accuaracy: 0.519\n",
      "Batch: 141 Epoch Loss: 4130301.47363 Batch Accuaracy: 0.5176\n",
      "Batch: 142 Epoch Loss: 4144932.16309 Batch Accuaracy: 0.5176\n",
      "Batch: 143 Epoch Loss: 4162509.60059 Batch Accuaracy: 0.5156\n",
      "Batch: 144 Epoch Loss: 4178182.65723 Batch Accuaracy: 0.5228\n",
      "Batch: 145 Epoch Loss: 4191007.98145 Batch Accuaracy: 0.527\n",
      "Batch: 146 Epoch Loss: 4207344.81738 Batch Accuaracy: 0.5306\n",
      "Batch: 147 Epoch Loss: 4220916.86523 Batch Accuaracy: 0.534\n",
      "Batch: 148 Epoch Loss: 4235732.12109 Batch Accuaracy: 0.536\n",
      "Batch: 149 Epoch Loss: 4250475.67969 Batch Accuaracy: 0.537\n",
      "Batch: 150 Epoch Loss: 4266647.25586 Batch Accuaracy: 0.538\n",
      "Batch: 151 Epoch Loss: 4281300.73242 Batch Accuaracy: 0.5388\n",
      "Batch: 152 Epoch Loss: 4295471.49609 Batch Accuaracy: 0.5432\n",
      "Batch: 153 Epoch Loss: 4310266.88477 Batch Accuaracy: 0.5448\n",
      "Batch: 154 Epoch Loss: 4323492.82422 Batch Accuaracy: 0.5472\n",
      "Batch: 155 Epoch Loss: 4336222.81836 Batch Accuaracy: 0.5534\n",
      "Batch: 156 Epoch Loss: 4351222.79688 Batch Accuaracy: 0.5536\n",
      "Batch: 157 Epoch Loss: 4367192.5332 Batch Accuaracy: 0.556\n",
      "Batch: 158 Epoch Loss: 4382381.03125 Batch Accuaracy: 0.556\n",
      "Batch: 159 Epoch Loss: 4395534.64551 Batch Accuaracy: 0.5552\n",
      "Batch: 160 Epoch Loss: 4409397.67578 Batch Accuaracy: 0.5554\n",
      "Batch: 161 Epoch Loss: 4429554.16992 Batch Accuaracy: 0.5554\n",
      "Batch: 162 Epoch Loss: 4445891.7793 Batch Accuaracy: 0.556\n",
      "Batch: 163 Epoch Loss: 4461422.57813 Batch Accuaracy: 0.5554\n",
      "Batch: 164 Epoch Loss: 4475179.71289 Batch Accuaracy: 0.5558\n",
      "Batch: 165 Epoch Loss: 4491244.69629 Batch Accuaracy: 0.5548\n",
      "Batch: 166 Epoch Loss: 4506211.67383 Batch Accuaracy: 0.553\n",
      "Batch: 167 Epoch Loss: 4521425.06543 Batch Accuaracy: 0.5504\n",
      "Batch: 168 Epoch Loss: 4537807.15723 Batch Accuaracy: 0.5474\n",
      "Batch: 169 Epoch Loss: 4550210.41016 Batch Accuaracy: 0.5456\n",
      "Batch: 170 Epoch Loss: 4564940.19434 Batch Accuaracy: 0.5448\n",
      "Batch: 171 Epoch Loss: 4578915.31348 Batch Accuaracy: 0.5438\n",
      "Batch: 172 Epoch Loss: 4590602.04395 Batch Accuaracy: 0.5462\n",
      "Batch: 173 Epoch Loss: 4608023.03223 Batch Accuaracy: 0.5496\n",
      "Batch: 174 Epoch Loss: 4625144.1416 Batch Accuaracy: 0.555\n",
      "Batch: 175 Epoch Loss: 4639971.02246 Batch Accuaracy: 0.5576\n",
      "Batch: 176 Epoch Loss: 4653297.12793 Batch Accuaracy: 0.559\n",
      "Batch: 177 Epoch Loss: 4666411.22949 Batch Accuaracy: 0.5608\n",
      "Batch: 178 Epoch Loss: 4677803.8252 Batch Accuaracy: 0.5598\n",
      "Batch: 179 Epoch Loss: 4690276.27734 Batch Accuaracy: 0.5644\n",
      "Batch: 180 Epoch Loss: 4707198.3125 Batch Accuaracy: 0.5654\n",
      "Batch: 181 Epoch Loss: 4720713.9834 Batch Accuaracy: 0.5654\n",
      "Batch: 182 Epoch Loss: 4732349.55469 Batch Accuaracy: 0.5666\n",
      "Batch: 183 Epoch Loss: 4747388.23145 Batch Accuaracy: 0.5716\n",
      "Batch: 184 Epoch Loss: 4761629.43457 Batch Accuaracy: 0.5708\n",
      "Batch: 185 Epoch Loss: 4774899.12109 Batch Accuaracy: 0.5744\n",
      "Batch: 186 Epoch Loss: 4786974.31055 Batch Accuaracy: 0.5732\n",
      "Batch: 187 Epoch Loss: 4802369.17285 Batch Accuaracy: 0.5742\n",
      "Batch: 188 Epoch Loss: 4818073.09766 Batch Accuaracy: 0.573\n",
      "Batch: 189 Epoch Loss: 4828229.59668 Batch Accuaracy: 0.5726\n",
      "Batch: 190 Epoch Loss: 4841582.64063 Batch Accuaracy: 0.5714\n",
      "Batch: 191 Epoch Loss: 4853463.11523 Batch Accuaracy: 0.5708\n",
      "Batch: 192 Epoch Loss: 4865877.06836 Batch Accuaracy: 0.5714\n",
      "Batch: 193 Epoch Loss: 4877314.31445 Batch Accuaracy: 0.5674\n",
      "Batch: 194 Epoch Loss: 4888014.94043 Batch Accuaracy: 0.5684\n",
      "Batch: 195 Epoch Loss: 4897311.63379 Batch Accuaracy: 0.5684\n",
      "Batch: 196 Epoch Loss: 4909282.54004 Batch Accuaracy: 0.5688\n",
      "Batch: 197 Epoch Loss: 4921941.43457 Batch Accuaracy: 0.5686\n",
      "Batch: 198 Epoch Loss: 4940601.15332 Batch Accuaracy: 0.5688\n",
      "Batch: 199 Epoch Loss: 4953607.74707 Batch Accuaracy: 0.5696\n",
      "Batch: 200 Epoch Loss: 4966111.53223 Batch Accuaracy: 0.5714\n",
      "Batch: 201 Epoch Loss: 4976364.53418 Batch Accuaracy: 0.5726\n",
      "Batch: 202 Epoch Loss: 4994243.47559 Batch Accuaracy: 0.5712\n",
      "Batch: 203 Epoch Loss: 5008136.06152 Batch Accuaracy: 0.5714\n",
      "Batch: 204 Epoch Loss: 5023882.89648 Batch Accuaracy: 0.5696\n",
      "Batch: 205 Epoch Loss: 5039934.8457 Batch Accuaracy: 0.57\n",
      "Batch: 206 Epoch Loss: 5053831.16016 Batch Accuaracy: 0.5702\n",
      "Batch: 207 Epoch Loss: 5069101.07324 Batch Accuaracy: 0.5728\n",
      "Batch: 208 Epoch Loss: 5087301.90723 Batch Accuaracy: 0.578\n",
      "Batch: 209 Epoch Loss: 5101750.82715 Batch Accuaracy: 0.5818\n",
      "Batch: 210 Epoch Loss: 5118068.03809 Batch Accuaracy: 0.5828\n",
      "Batch: 211 Epoch Loss: 5134059.94238 Batch Accuaracy: 0.5872\n",
      "Batch: 212 Epoch Loss: 5146554.27148 Batch Accuaracy: 0.5952\n",
      "Batch: 213 Epoch Loss: 5159108.48145 Batch Accuaracy: 0.5998\n",
      "Batch: 214 Epoch Loss: 5170094.7373 Batch Accuaracy: 0.6024\n",
      "Batch: 215 Epoch Loss: 5182721.46582 Batch Accuaracy: 0.6004\n",
      "Batch: 216 Epoch Loss: 5193917.8252 Batch Accuaracy: 0.6004\n",
      "Batch: 217 Epoch Loss: 5203071.09668 Batch Accuaracy: 0.602\n",
      "Batch: 218 Epoch Loss: 5219315.74219 Batch Accuaracy: 0.6034\n",
      "Batch: 219 Epoch Loss: 5227319.71973 Batch Accuaracy: 0.6018\n",
      "Batch: 220 Epoch Loss: 5239317.88184 Batch Accuaracy: 0.6022\n",
      "Batch: 221 Epoch Loss: 5249747.78711 Batch Accuaracy: 0.6\n",
      "Batch: 222 Epoch Loss: 5260104.66602 Batch Accuaracy: 0.5992\n",
      "Batch: 223 Epoch Loss: 5273386.31445 Batch Accuaracy: 0.6006\n",
      "Batch: 224 Epoch Loss: 5285146.5332 Batch Accuaracy: 0.5984\n",
      "Batch: 225 Epoch Loss: 5297078.5332 Batch Accuaracy: 0.595\n",
      "Batch: 226 Epoch Loss: 5310163.62109 Batch Accuaracy: 0.5918\n",
      "Batch: 227 Epoch Loss: 5324780.48047 Batch Accuaracy: 0.5874\n",
      "Batch: 228 Epoch Loss: 5333392.02734 Batch Accuaracy: 0.5876\n",
      "Batch: 229 Epoch Loss: 5344649.95996 Batch Accuaracy: 0.5888\n",
      "Batch: 230 Epoch Loss: 5358623.98535 Batch Accuaracy: 0.5908\n",
      "Batch: 231 Epoch Loss: 5368771.67285 Batch Accuaracy: 0.5954\n",
      "Batch: 232 Epoch Loss: 5379936.08105 Batch Accuaracy: 0.5986\n",
      "Batch: 233 Epoch Loss: 5390984.78809 Batch Accuaracy: 0.6008\n",
      "Batch: 234 Epoch Loss: 5401539.07129 Batch Accuaracy: 0.6028\n",
      "Batch: 235 Epoch Loss: 5412048.61133 Batch Accuaracy: 0.605\n",
      "Batch: 236 Epoch Loss: 5422833.60547 Batch Accuaracy: 0.6084\n",
      "Batch: 237 Epoch Loss: 5433612.74512 Batch Accuaracy: 0.6086\n",
      "Batch: 238 Epoch Loss: 5444203.09863 Batch Accuaracy: 0.6092\n",
      "Batch: 239 Epoch Loss: 5459535.38867 Batch Accuaracy: 0.6092\n",
      "Batch: 240 Epoch Loss: 5472845.68164 Batch Accuaracy: 0.608\n",
      "Batch: 241 Epoch Loss: 5489224.20313 Batch Accuaracy: 0.6064\n",
      "Batch: 242 Epoch Loss: 5502049.27148 Batch Accuaracy: 0.6042\n",
      "Batch: 243 Epoch Loss: 5513342.16406 Batch Accuaracy: 0.606\n",
      "Batch: 244 Epoch Loss: 5525223.08398 Batch Accuaracy: 0.6054\n",
      "Batch: 245 Epoch Loss: 5535648.28516 Batch Accuaracy: 0.6014\n",
      "Batch: 246 Epoch Loss: 5548320.7832 Batch Accuaracy: 0.6004\n",
      "Batch: 247 Epoch Loss: 5559153.59375 Batch Accuaracy: 0.604\n",
      "Batch: 248 Epoch Loss: 5568893.81641 Batch Accuaracy: 0.609\n",
      "Batch: 249 Epoch Loss: 5585805.57422 Batch Accuaracy: 0.6122\n",
      "Batch: 250 Epoch Loss: 5596731.16016 Batch Accuaracy: 0.6102\n",
      "Batch: 251 Epoch Loss: 5608342.25781 Batch Accuaracy: 0.6088\n",
      "Batch: 252 Epoch Loss: 5621397.58691 Batch Accuaracy: 0.6102\n",
      "Batch: 253 Epoch Loss: 5632240.14355 Batch Accuaracy: 0.6062\n",
      "Batch: 254 Epoch Loss: 5642921.15527 Batch Accuaracy: 0.6034\n",
      "Batch: 255 Epoch Loss: 5654993.10742 Batch Accuaracy: 0.601\n",
      "Batch: 256 Epoch Loss: 5666204.56738 Batch Accuaracy: 0.5996\n",
      "Batch: 257 Epoch Loss: 5675309.46582 Batch Accuaracy: 0.601\n",
      "Batch: 258 Epoch Loss: 5684799.6416 Batch Accuaracy: 0.6024\n",
      "Batch: 259 Epoch Loss: 5696102.08301 Batch Accuaracy: 0.6022\n",
      "Batch: 260 Epoch Loss: 5707453.50488 Batch Accuaracy: 0.6016\n",
      "Batch: 261 Epoch Loss: 5718193.24512 Batch Accuaracy: 0.6036\n",
      "Batch: 262 Epoch Loss: 5728742.1582 Batch Accuaracy: 0.6098\n",
      "Batch: 263 Epoch Loss: 5743347.45117 Batch Accuaracy: 0.6138\n",
      "Batch: 264 Epoch Loss: 5756594.87012 Batch Accuaracy: 0.6164\n",
      "Batch: 265 Epoch Loss: 5765976.50879 Batch Accuaracy: 0.6208\n",
      "Batch: 266 Epoch Loss: 5779008.49316 Batch Accuaracy: 0.625\n",
      "Batch: 267 Epoch Loss: 5792462.71582 Batch Accuaracy: 0.6264\n",
      "Batch: 268 Epoch Loss: 5802074.69043 Batch Accuaracy: 0.6282\n",
      "Batch: 269 Epoch Loss: 5814099.61426 Batch Accuaracy: 0.6302\n",
      "Batch: 270 Epoch Loss: 5824924.92285 Batch Accuaracy: 0.6268\n",
      "Epoch: 0 Epoch Loss: 5824924.92285 Validation Accuaracy: 0.6268\n"
     ]
    }
   ],
   "source": [
    "saver=tf.train.Saver()\n",
    "train_neural_network(train_X,train_y,saver)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
